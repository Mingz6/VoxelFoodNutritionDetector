{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ‰ Welcome to the FiftyOne Project in Google Colab! ðŸŽ‰\n",
        "ðŸš€ **FiftyOne** is an open-source toolkit for exploring, visualizing, and managing your datasets, making it easier to work with images and machine learning workflows.\n",
        "\n",
        "## ðŸ”¹ Features:\n",
        "âœ… Easily browse and filter large datasets  \n",
        "âœ… Powerful visualization tools for images and annotations  \n",
        "âœ… Integration with deep learning frameworks like TensorFlow and PyTorch  \n",
        "\n",
        "## ðŸ“Œ Setup Instructions:\n",
        "Run the following command to install FiftyOne if you haven't already:  \n",
        "```python\n",
        "!pip install fiftyone\n",
        "!pio install databases\n",
        "\n"
      ],
      "metadata": {
        "id": "aYUMFbYdKXn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import fiftyone as fo\n",
        "    print(\"âœ… FiftyOne is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"ðŸ“¦ Installing FiftyOne...\")\n",
        "    %pip install -q fiftyone\n",
        "\n",
        "# Verify installation\n",
        "!pip list | grep -i fiftyone\n",
        "\n",
        "try:\n",
        "    import datasets\n",
        "    print(\"âœ… datasets is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"ðŸ“¦ Installing datasets...\")\n",
        "    %pip install -q datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face using datasets\n",
        "hf_dataset = load_dataset(\"TeeA/nutrition5k-food-name-gemini\")\n",
        "\n",
        "# To check the data being succesffully loaded\n",
        "# hf_dataset['train'][0]\n",
        "# dir(hf_dataset['train'])"
      ],
      "metadata": {
        "id": "PQeMPS29IRFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6d4d9e-6638-4bd1-dd40-389084e51d3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… FiftyOne is already installed.\n",
            "fiftyone                           1.3.2\n",
            "fiftyone-brain                     0.19.0\n",
            "fiftyone_db                        1.1.7\n",
            "âœ… datasets is already installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "# Define the directory to save images\n",
        "image_dir = \"images\"\n",
        "\n",
        "# Clean up the image folder before adding new images\n",
        "if os.path.exists(image_dir):\n",
        "    shutil.rmtree(image_dir)  # Remove all files and the folder\n",
        "os.makedirs(image_dir, exist_ok=True)  # Recreate an empty directory\n",
        "\n",
        "# Create a FiftyOne dataset\n",
        "dataset = fo.Dataset(\"food_dataset\", overwrite=True)\n",
        "\n",
        "# Create a list to store samples\n",
        "samples = []\n",
        "\n",
        "# Set max_images to None or 0 to process all images\n",
        "max_images = 10  # Change this to an integer to limit, or None to process all\n",
        "\n",
        "# Iterate through dataset and limit images if max_images is set\n",
        "for i, data in enumerate(hf_dataset['train']):\n",
        "    if max_images and i >= max_images:\n",
        "        break  # Stop after reaching the limit\n",
        "\n",
        "    # Save the image to disk\n",
        "    image_path = os.path.join(image_dir, f\"{data['dish_id']}.png\")\n",
        "    data['dish_image'].save(image_path)\n",
        "\n",
        "    # Create a FiftyOne sample\n",
        "    sample = fo.Sample(filepath=image_path)\n",
        "    sample['dish_id'] = data['dish_id']\n",
        "    sample['food_name'] = fo.Classification(label=data['food_name'].strip())\n",
        "\n",
        "    # Add the sample to the list\n",
        "    samples.append(sample)\n",
        "\n",
        "# Add samples to the dataset\n",
        "dataset.add_samples(samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c-1ofpC5USDC",
        "outputId": "62787add-490e-4a9e-b9da-aef986621e7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [67.5ms elapsed, 0s remaining, 148.2 samples/s]     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [67.5ms elapsed, 0s remaining, 148.2 samples/s]     \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['67d5ff3145a926d493605870',\n",
              " '67d5ff3145a926d493605871',\n",
              " '67d5ff3145a926d493605872',\n",
              " '67d5ff3145a926d493605873',\n",
              " '67d5ff3145a926d493605874',\n",
              " '67d5ff3145a926d493605875',\n",
              " '67d5ff3145a926d493605876',\n",
              " '67d5ff3145a926d493605877',\n",
              " '67d5ff3145a926d493605878',\n",
              " '67d5ff3145a926d493605879']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset_filtered = fo.Dataset.from_images_dir(\"images\")\n",
        "session = fo.launch_app(hf_dataset_filtered)"
      ],
      "metadata": {
        "id": "L4Yi88ReY2NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset_filtered\n"
      ],
      "metadata": {
        "id": "KjXSKuxSuHVs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}