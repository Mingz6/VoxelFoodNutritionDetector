{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Welcome to the FiftyOne Project in Google Colab! ðŸŽ‰\n",
    "ðŸš€ **FiftyOne** is an open-source toolkit for exploring, visualizing, and managing your datasets, making it easier to work with images and machine learning workflows.\n",
    "\n",
    "## ðŸ”¹ Features:\n",
    "âœ… Easily browse and filter large datasets  \n",
    "âœ… Powerful visualization tools for images and annotations  \n",
    "âœ… Integration with deep learning frameworks like TensorFlow and PyTorch  \n",
    "\n",
    "## ðŸ“Œ Setup Instructions:\n",
    "Run the following command to install FiftyOne if you haven't already:  \n",
    "```python\n",
    "!pip install fiftyone\n",
    "!pio install databases\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import fiftyone as fo\n",
    "    print(\"âœ… FiftyOne is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"ðŸ“¦ Installing FiftyOne...\")\n",
    "    %pip install -q fiftyone\n",
    "\n",
    "# Verify installation\n",
    "!pip list | grep -i fiftyone\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(\"âœ… datasets is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"ðŸ“¦ Installing datasets...\")\n",
    "    %pip install -q datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from HfToken import HfToken\n",
    "\n",
    "login(HfToken)\n",
    "\n",
    "# Load the dataset from Hugging Face using datasets\n",
    "hf_dataset = load_dataset(\"TeeA/nutrition5k-food-name-gemini\")\n",
    "\n",
    "# To check the data being succesffully loaded\n",
    "hf_dataset['train'][0]\n",
    "# dir(hf_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Define the directory to save images\n",
    "image_dir = \"images\"\n",
    "\n",
    "# Clean up the image folder before adding new images\n",
    "if os.path.exists(image_dir):\n",
    "    shutil.rmtree(image_dir)  # Remove all files and the folder\n",
    "os.makedirs(image_dir, exist_ok=True)  # Recreate an empty directory\n",
    "\n",
    "# Create a FiftyOne dataset\n",
    "dataset = fo.Dataset(\"food_dataset\", overwrite=True)\n",
    "\n",
    "# Create a list to store samples\n",
    "samples = []\n",
    "\n",
    "# Set max_images to None or 0 to process all images\n",
    "max_images = 10  # Change this to an integer to limit, or None to process all\n",
    "\n",
    "# Iterate through dataset and limit images if max_images is set\n",
    "for i, data in enumerate(hf_dataset['train']):\n",
    "    if max_images and i >= max_images:\n",
    "        break  # Stop after reaching the limit\n",
    "\n",
    "    # Save the image to disk\n",
    "    image_path = os.path.join(image_dir, f\"{data['dish_id']}.png\")\n",
    "    data['dish_image'].save(image_path)\n",
    "\n",
    "    # Create a FiftyOne sample\n",
    "    sample = fo.Sample(filepath=image_path)\n",
    "    sample['dish_id'] = data['dish_id']\n",
    "    sample['food_name'] = fo.Classification(label=data['food_name'].strip())\n",
    "\n",
    "    # Add the sample to the list\n",
    "    samples.append(sample)\n",
    "\n",
    "# Add samples to the dataset\n",
    "dataset.add_samples(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hf_dataset_filtered = fo.Dataset.from_images_dir(\"images\")\n",
    "session = fo.launch_app(hf_dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hf_dataset_filtered\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
